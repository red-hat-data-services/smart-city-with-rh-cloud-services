= Smart Cities Workshop Deployment Guide
:sectnums:
:sectnumlevels: 2
:toc:

== Requirements

TODO: Review

* A working OpenShift environment. Last tested version: 4.9
* Cluster-admin access to OpenShift, or ability to deploy operators.
* The image-registry Operator needs to be setup and working
* The `oc` client.
* Optional: the `jq` tool (JSON patcher).

== Automated install

TODO: To be rewritten.

== Manual install

NOTE: In those instructions, all commands are run from the `deployment` folder.

=== Namespace

Create an OpenShift project/namespace to deploy the environment. In this documentation we'll use `smartcity`.

[source,bash]
----
oc new-project smartcity
----

WARNING: If you did not use `smartcity` as the name of your project, don't forget to change it in the commands or the files used for the deployment.

=== Database

We will need a database to store informations about the workflow, as well as registration information for the vehicles. You can edit the Secret file if you want change the default values.

TODO: Instructions for RHODA

.Seed the database
[source,bash]
----
oc apply -f seed_database/job_seed_database.yaml
----

=== Kafka

==== GUI deployment 

We will use Red Hat OpenShift Streams for Apache Kafka (RHOSAK), a managed cloud service from Red Hat, so we can quickly get started without focusing on the installation and self-management of Kafka clusters.

Red Hat OpenShift Streams for Apache Kafka's product documentation page contains the latest information on this service and is the source of truth for RHOSAK. We will modify excerpts from the following documentation links below. 

*Links:*

* https://access.redhat.com/documentation/en-us/red_hat_openshift_streams_for_apache_kafka/1[Product documentation]
* https://access.redhat.com/documentation/en-us/red_hat_openshift_streams_for_apache_kafka/1/guide/f351c4bd-9840-42ef-bcf2-b0c9be4ee30a[Getting started with RHOSAK] 

In this demo, we will simulate both "Edge" and "Core" kafka instances using a single RHOSAK Kafka instance. 

===== Kafka instance

1. Log into Red Hat OpenShift Streams for Apache Kafa at https://console.redhat.com/application-services/streams/
2. This should bring you to the right screen, but if it doesn't, go to *Streams for Apache Kafka* > *Kafka Instances* and click *Create Kafka Instance* 
3. Enter the name as `sc-kafka`, select a region, and click *Create instance*. 
4. Wait a few minutes until *Status* is *Ready*.

===== Create lpr topic

1. Once your Kafka isntance's *Status* is *Ready*, click the instance name (`sc-kafka` hyperlink) and select *Topics* from the horizontal tabs at the top. 
2. Select *Create topic*, enter `lpr` as your *Topic name* and click *Next*.
3. Increase partitions to `3` using the plus sign and select *Next*.
4. Select a *Retention time* of *A day* with size *Unlimited*. (Keep in mind, RHOSAK free instances are only available for 2 days). Click *Next*. 
5. *Replicas* should be automatically set to 3 with *Minimum in-sync replicas* set to 2. Click *Finish*. 

===== Copy connection details

Red Hat OpenShift Streams for Apache Kafka instances use the *bootstrap server* to create a connection. We authenticate connections using *service account credentials*. This section shows you where to get this information so you can secure your applications. 

1. Return to the *Kafka Instances* (on the left, *Streams for Apache Kafka* > *Kafka Instances*) view and select the vertical dots on the right side of your kafka instance listing. Select *Connection*.
2. Under *Service accounts*, select *Create service account*. 
3. Provide a short description, i.e.`sa-kafka-smart-city-lpr` and select *Create* 
4. Copy and save both the *Client ID* and the *Client secret*. You will need these later. 
5. Check *I have copied the client ID and secret* and click *Close*. 
6. Before closing the pane, copy and save the *Bootstrap server* details. 
7. Copy and save *Token endpoint URL* before closing the page.

////
*Create lpr-core topic:* 
1. Follow instructions above to create a second topic. Use the same configurations while substituting the topic name for `lpr-core`

*Connection information:*
1. Return to the *Kafka Instances* by selecting *Kafka Instances* on left side of your screen
2. Find your Kafka 

////

===== Add Secrets to OpenShift

We will now create Secrets using the connection information copied in the previous step. We will edit a copied template file using the following steps.

1. From the directory containing this README file, run: 
[source,bash]
----
cp deployment/files/kafka/rhosak-secret.yaml deployment/files/kafka/rhosak-secret.yaml.env
----
2. Use an editor to relace the relevant connection strings (`ENTER_*`) in `deployment/files/kafka/rhosak-secret.yaml.env`. 
3. Use oc to apply changes
[source,bash]
----
oc apply -f deployment/files/kafka/rhosak-secret.yaml.env
----

===== Using connection secrets in our application

We pushed our RHOSAK connection information to our OpenShift cluster in the last section. We can now use this information to securely connect our Kafka instance to our Smart City application. 

The bootstrap server is used to make an initial connection to our RHOSAK Kafka instance. There are two mechanisms for authentication:

1. *SASL/OAUTHBEARER* (_recommended_)
2. *SASL/PLAIN* 

*SASL/OAUTHBEARER* uses the service account we created and the token endpoint url to authenticate our smarty city application with the Kafka instance. *SASL/PLAIN* uses our service account Client ID and Client Secret strings as username and password credentials. 

//We will use SASL/OAUTHBEARER to connect to this instance.

=== LPR Service

This component presents an API that you can query by sending an image. It will return the infered licence plate number.

.Create the LPR deployment and service
[source,bash]
----
oc apply -f lpr_service/deployment_lpr_service.yaml
oc apply -f lpr_service/svc_lpr_service.yaml
----

=== Events Service

This is the component that runs in the Core and listens to incoming Kafka events to write them into a PostgreSQL database so that they can be queried to create the dashboards.

.Create Events Service
[source,bash]
----
oc apply -f lpr_service/deployment_lpr_service.yaml
----

=== Image Server

This component will return the image of the last identified vehicle to be displayed on the dashbord.

- Get the RGW Endpoint Name and update `image_server/dc_image-server.yaml`
```
export RGW_ROUTE=https://$(oc get routes -n openshift-storage | grep rgw | awk '{ print $2 }')
sed -i 's@RGW_SERVICE_ENDPOINT@'$RGW_ROUTE'@' image_server/dc_image-server.yaml
```

* `image_server/is_image-server.yaml`: ImageStream for the image-server
* `image_server/bc_image-server.yaml`: Build Config for the image-server
* `image_server/dc_image-server.yaml`: Deployment Config/Service/Route for the image-server

=== Load Generator

This is the component that injects car images into the pipeline.

* `generator/obc_dataset_generator.yaml`: Bucket to store the images dataset
* `generator/is_generator.yaml`: ImageStream for the load generator
* `generator/bc_generator.yaml`: BuildConfiguration to create the load generator image
* `generator/dc_generator.yaml`: Deployment Configuration for the load generator

=== Ceph Nano

We will use Ceph Nano to provide object storage to hold the source images that will be injected in the pipeline, and the events that will be generated.

.Deploy Ceh Nano
[source,bash]
----
oc apply -k ceph_nano/scc
oc apply -k ceph_nano/nano
----

=== Dataset

Retrieve the information for the dataset bucket created previously and upload the images.

[source,bash]
----
export AWS_ACCESS_KEY_ID=$(oc get secret/generator-dataset -o yaml | grep " AWS_ACCESS_KEY_ID" | awk '{ print $2 }' - | base64 -d)
export AWS_SECRET_ACCESS_KEY=$(oc get secret/generator-dataset -o yaml | grep " AWS_SECRET_ACCESS_KEY" | awk '{ print $2 }' - | base64 -d)
export RGW_ROUTE=https://$(oc get routes -n openshift-storage | grep rgw | awk '{ print $2 }')
export BUCKET=$(oc get cm/generator-dataset -o yaml | grep " BUCKET_NAME:" | awk '{ print $2 }' -)
aws --endpoint-url $RGW_ROUTE s3 cp --recursive ../source/dataset/images s3://$BUCKET/images
----

This bucket also has to be made readable to display the images.

.Apply the anonymous readonly policy
[source,bash]
----
sed 's/MY_BUCKET/'$BUCKET'/' image_server/policy.json > /tmp/policy.json && aws --endpoint-url $RGW_ROUTE s3api put-bucket-policy --bucket $BUCKET --policy file:///tmp/policy.json
----

=== Start Traffic

By default `generator` has no pods running, in order to simulate traffic, you will increase the replica count of generator deployment to `1` (not yet, after you have deployed all the components!)

[source,bash]
----
oc scale dc/generator --replicas 1
----

Verify the generated traffic by visiting the following kafdrop URL for edge and core kafka clusters
[source,bash]
----
echo "http://$(oc get route | grep -i edge-kafdrop | awk '{print $2}')/topic/lpr/messages?partition=0&offset=0&count=100&keyFormat=DEFAULT&format=DEFAULT"
echo "http://$(oc get route | grep -i core-kafdrop | awk '{print $2}')/topic/lpr/messages?partition=0&offset=0&count=100&keyFormat=DEFAULT&format=DEFAULT"
----

=== Fee Calculation

For calculating the toll and pollution fee, there are two cases that we have covered:

* When any vehicle enters the ULEZ, a certain fee (aka toll fee) must be applied to that vehicle
* If the vehicle model is too old (older than 2014), apply addition fee (aka pollution fee) on that vehicle

Deploy the fee calculation component, using the following commands

[source,bash]
----
oc create -f fee_calculation/is_fee_calculation.yaml
oc create -f fee_calculation/bc_fee_calculation.yaml
oc create -f fee_calculation/cronjob_fee_calculation.yaml
----

=== Secor

Secor is the component that will listen to the Kafka Stream and write the aggregated data to an object Bucket.

* `secor/1_obc_secor.yaml`: Bucket to store the streamed data
* `secor/2_zookeeper_entrance.yaml`: Connection to the Kafka-Core instance
* `secor/3_secor.yaml`: Deploys the Secor instance

=== Superset - Trino

TODO: Manual deployments

Open Data Hub will allow us to easily deploy SuperSet and Trino.

IMPORTANT: Before you apply `opendatahub/kfdef.yaml` make sure to replace s3 endpoint with RWG IP, using the following command

// TODO: Check the s3.data.local deployment

[source,bash]
----
RGW_IP=$(oc get svc -n openshift-storage | grep -i rgw | awk '{print $3}')
sed -i 's/s3.data.local/'$RGW_IP'/g' opendatahub/kfdef.yaml
----

* `opendatahub/kfdef.yaml`: Deploys an Open Data Hub instance with the needed components

Once the components are running (check the pods!) you can connect to the ODH dasboard to launch Superset or Grafana. The Route can be found in the OpenShift UI or like this:

[source,bash]
----
echo "https://$(oc get route | grep -i odh-dashboard | awk '{print $2}')"
----

==== Superset

* For superset to establish connection with PostgreSQL, set the credentials in `superset-dasboard.yaml` file

[source, bash]
----
sed -i "s/DB_USER/dbadmin/" superset/config/superset-datasources.yaml
sed -i "s/DB_PASSWORD/dbpassword/" superset/config/superset-datasources.yaml
sed -i "s/DB_NAME/pgdb/" superset/config/superset-datasources.yaml
----

* Transfer the DataSources configuration file into the Superset pod.

[source,bash]
----
oc rsync superset/config $(oc get pod | grep superset- | awk '{print $1}'):/tmp
----

* Import the datasources into Superset (PostgreSQL and Hive from Trino)

[source,bash]
----
oc exec $(oc get pod | grep superset- | awk '{print $1}') -- superset import_datasources -p /tmp/config/superset-datasources.yaml
----

* Log into Superset you can use admin / admin (unless you have modified it into the ODH KfDef).
* From the Settings menu (top right), import the example dasboard from the file `dashboard/dashboard.json`

==== Trino

Once the trino-coordinator pod is running, connect to trino using trino-cli

[source,bash]
----
wget https://repo1.maven.org/maven2/io/trino/trino-cli/358/trino-cli-358-executable.jar -O trino
chmod +x trino
oc port-forward svc/trino-service 8080:8080
./trino --server localhost:8080 --catalog hive --schema default
----

From the Trino prompt, create schema and table

IMPORTANT: Before you execute the command to create schema and table , make sure to replace the bucket name with your bucket. To grab bucket name execute `oc get obc secor-obc -o json | jq -r .spec.bucketName`

[source,sql]
----
CREATE SCHEMA hive.odf WITH (location = 's3a://replace_with_secor_bucket_name/');

CREATE TABLE IF NOT EXISTS hive.odf.event(event_timestamp timestamp, event_id varchar, event_vehicle_detected_plate_number varchar, event_vehicle_detected_lat varchar, event_vehicle_detected_long varchar, event_vehicle_lpn_detection_status varchar, stationa1 boolean, stationa5201 boolean, stationa13 boolean, stationa2 boolean, stationa23 boolean, stationb313 boolean, stationa4202 boolean, stationa41 boolean, stationb504 boolean, dt varchar) with ( external_location = 's3a://replace_with_secor_bucket_name/raw_logs/lpr/', format = 'ORC', partitioned_by=ARRAY['dt']);

CALL system.sync_partition_metadata(schema_name=>'odf', table_name=>'event', mode=>'FULL');

SELECT event_timestamp,event_vehicle_detected_plate_number,event_vehicle_lpn_detection_status FROM hive.odf.event LIMIT 10;
----


=== Grafana

Grafana will allow us to create dashbord to visualize the data workflow (Ops dashboard) and the Business Application itself (Main dashboard). All the deployments are taken care of by the Grafana operator deployed previously (see requirements).

* PGSQL Source to retrieve the events and vehicle data

.Retrieve the secrets, process the template, and apply the configuration
[source,bash]
----
oc process -f grafana/grafana-pgsql-datasource.yaml -p db_database=$(oc get secret/postgresql -o yaml | grep " database-name:" | awk '{ print $2 }' - | base64 -d) -p db_user=$(oc get secret/postgresql -o yaml | grep " database-user:" | awk '{ print $2 }' - | base64 -d) -p db_password=$(oc get secret/postgresql -o yaml | grep " database-password:" | awk '{ print $2 }' - | base64 -d) | oc apply -f -
----

* Prometheus Data Source to retrieve the CPU and RAM metrics

Our Grafana dashboard wil connect to the main OpenShift Prometheus instance to retrieve CPU and RAM information. To enable this, follow those steps:

.Grant the Grafana Service Account the cluster-monitoring-view cluster role:
[source,bash]
----
oc adm policy add-cluster-role-to-user cluster-monitoring-view -z grafana-serviceaccount
----

.Retrieve the bearer token used to authenticate to Prometheus:
[source,bash]
----
export bearer_token=$(oc serviceaccounts get-token grafana-serviceaccount)
----

.Deploy the Prometheus data source by using the template and substituting the bearer token:
[source,bash]
----
sed 's/BEARER_TOKEN/'$bearer_token'/' grafana/grafana-prometheus-datasource.yaml | oc apply -f -
----

You can now apply the two last files:

* Main application dashboard

.Retrieve the image server url, process the template, and apply the configuration
[source,bash]
----
oc process -f grafana/grafana-main-dashboard.yaml -p image_server_host=$(oc get route | grep -i image-server | awk '{print $2}') | oc apply -f -
----

* `grafana/grafana-pipeline-cpu-dashboard.yaml`: CPU Ops dashboard
* `grafana/grafana-pipeline-ram-dashboard.yaml`: RAM Ops dashboard
>>>>>>> main
