= Smart Cities Workshop Deployment Guide
:sectnums:
:sectnumlevels: 2
:toc:

== Requirements

TODO: Review
* A working OpenShift environment. Last tested version: 4.9
* Cluster-admin access to OpenShift, or ability to deploy operators.
* The image-registry Operator needs to be setup and working
* The `oc` client.
* Optional: the `jq` tool (JSON patcher).

== Automated install

TODO: To be rewritten.

== Manual install

=== Namespace

Create an OpenShift project/namespace to deploy the environment. In this documentation we'll use `smartcity`.

[source,bash]
----
oc new-project smartcity
----

TIP: If you did not use `smartcity` as the name of your project, don't forget to change it in the commands or the config files used for the deployment.

=== Deployment

From the `deploy` folder and subfolders, create the OpenShift resources in this order.

.Creating a resource
[source,bash]
----
oc apply -f file.yaml
----

=== Database

We will need a database to store informations about the workflow, as well as registration information for the vehicles. You can edit the Secret file if you want change the default values.

Deploying PostgreSQL DB

TODO: Instructions for RHODA

Deploying Seed Database to initialize the database with the registration informations.

* `database/seed_database/is_seed_database.yaml`: ImageStream for the image that will be used to see the DB
* `database/seed_database/bc_seed_database.yaml`: BuildConfiguration for the image

IMPORTANT: Before you apply `job_seed_database.yaml` make sure the build process (from the last step) has been completed, else seed job will complain until the image is not ready.

* `database/seed_database/job_seed_database.yaml`: Seeding Job to initialize the DB

=== Kafka

TODO: RHOSAK instructions to create topics and Mirror Maker (possible?)

* `kafka/edge-topic.yaml`: Edge topic
* `kafka/core-topic.yaml`: Core topic
* `kafka/mirror-maker.yaml`: Mirror maker

==== GUI deployment 

We will use Red Hat OpenShift Streams for Apache Kafka (RHOSAK), a managed cloud service from Red Hat, so we can quickly get started without focusing on the installation and self-management of Kafka clusters.

Red Hat OpenShift Streams for Apache Kafka's product documentation page contains the latest information on this service and is the source of truth for RHOSAK. We will modify excerpts from the following documentation links below. 

*Links:*

* https://access.redhat.com/documentation/en-us/red_hat_openshift_streams_for_apache_kafka/1[Product documentation]
* https://access.redhat.com/documentation/en-us/red_hat_openshift_streams_for_apache_kafka/1/guide/f351c4bd-9840-42ef-bcf2-b0c9be4ee30a[Getting started with RHOSAK] 

In this demo, we will simulate both "Edge" and "Core" kafka instances using a single RHOSAK Kafka instance. 

*Kafka instance:*

0. Log into Red Hat OpenShift Streams for Apache Kafa at https://console.redhat.com/application-services/streams/
1. Go to *Streams for Apache Kafka* > *Kafka Instances* and click *Create Kafka Instance* 
2. Enter the name as `core-kafka` and select a region. 
3. Click *Create instance*. 
4. Wait a few minutes until *Status* is *Ready*.

*Create lpr topic:*

1. Once your Kafka isntance's *Status* is *Ready*, click the instance name and select *Topics* from the horizontal tabs at the top. 
2. Select "Create topic".
3. On the *Create topic* screen, enter a *Topic name* and click *Next*. We are naming our topic: `lpr`
4. Increase partitions to 3 using the plus sign and select *Next*.
5. Select a *Retention time* of *A day* with size *Unlimited*. (Keep in mind, RHOSAK free instances are only available for 2 days). Click *Next*. 
6. *Replicas* should be automatically set to 3 with *Minimum in-sync replicas* set to 2. Click *Finish*. 

*Copy connection details* 

0. Return to the *Kafka Instances* view and select the vertical dots on the right side of your kafka instance listing. Select *Connection*.

_Create a Service Account:_
1. Under *Service accounts*, select *Create service account*. 
2. Provide a short description, i.e.`sa-kafka-smart-city-lpr` and select *Create* 
3. Copy and save both the *Client ID* and the *Client secret*. You will need these later. 
4. Check *I have copied the client ID and secret* and click *Close*. 

_Copy bootstrap server  & token endpoint info:_
5. Before closing the pane, copy and save the *Bootstrap server* details. 
6. Copy and save *Token endpoint URL* before closing the page.

////
*Create lpr-core topic:* 
1. Follow instructions above to create a second topic. Use the same configurations while substituting the topic name for `lpr-core`

*Connection information:*
1. Return to the *Kafka Instances* by selecting *Kafka Instances* on left side of your screen
2. Find your Kafka 

////


=== LPR Service

This component presents an API that you can query with an image and returns the infered licence plate number.

* `lpr_service/is_lpr_service.yaml`: ImageStream for the LPR service
* `lpr_service/bc_lpr_service.yaml`: BuildConfiguration for the LPR service
* `lpr_service/dc_lpr_service.yaml`: Deployment Configuration for the LPR service
* `lpr_service/svc_lpr_service.yaml`: Service to access the LPR service


=== Events Service

This is the component that runs in the Core and listens to incoming Kafka events to write them into a PostgreSQL database so that they can be queried to create the dashboards.

* `events_service/is_events_service.yaml`: ImageStream for the event service
* `events_service/bc_events_service.yaml`: BuildConfiguration for the event service
* `events_service/dc_events_service.yaml`: Deployment Configuration for the event service

=== Image Server

This component will return the image of the last identified vehicle to be displayed on the dashbord.

- Get the RGW Endpoint Name and update `image_server/dc_image-server.yaml`
```
export RGW_ROUTE=https://$(oc get routes -n openshift-storage | grep rgw | awk '{ print $2 }')
sed -i 's@RGW_SERVICE_ENDPOINT@'$RGW_ROUTE'@' image_server/dc_image-server.yaml
```

* `image_server/is_image-server.yaml`: ImageStream for the image-server
* `image_server/bc_image-server.yaml`: Build Config for the image-server
* `image_server/dc_image-server.yaml`: Deployment Config/Service/Route for the image-server

=== Load Generator

This is the component that injects car images into the pipeline.

* `generator/obc_dataset_generator.yaml`: Bucket to store the images dataset
* `generator/is_generator.yaml`: ImageStream for the load generator
* `generator/bc_generator.yaml`: BuildConfiguration to create the load generator image
* `generator/dc_generator.yaml`: Deployment Configuration for the load generator

=== Dataset

Retrieve the information for the dataset bucket created previously and upload the images.

[source,bash]
----
export AWS_ACCESS_KEY_ID=$(oc get secret/generator-dataset -o yaml | grep " AWS_ACCESS_KEY_ID" | awk '{ print $2 }' - | base64 -d)
export AWS_SECRET_ACCESS_KEY=$(oc get secret/generator-dataset -o yaml | grep " AWS_SECRET_ACCESS_KEY" | awk '{ print $2 }' - | base64 -d)
export RGW_ROUTE=https://$(oc get routes -n openshift-storage | grep rgw | awk '{ print $2 }')
export BUCKET=$(oc get cm/generator-dataset -o yaml | grep " BUCKET_NAME:" | awk '{ print $2 }' -)
aws --endpoint-url $RGW_ROUTE s3 cp --recursive ../source/dataset/images s3://$BUCKET/images
----

This bucket also has to be made readable to display the images.

.Apply the anonymous readonly policy
[source,bash]
----
sed 's/MY_BUCKET/'$BUCKET'/' image_server/policy.json > /tmp/policy.json && aws --endpoint-url $RGW_ROUTE s3api put-bucket-policy --bucket $BUCKET --policy file:///tmp/policy.json
----

=== Start Traffic

By default `generator` has no pods running, in order to simulate traffic, you will increase the replica count of generator deployment to `1` (not yet, after you have deployed all the components!)

[source,bash]
----
oc scale dc/generator --replicas 1
----

Verify the generated traffic by visiting the following kafdrop URL for edge and core kafka clusters
[source,bash]
----
echo "http://$(oc get route | grep -i edge-kafdrop | awk '{print $2}')/topic/lpr/messages?partition=0&offset=0&count=100&keyFormat=DEFAULT&format=DEFAULT"
echo "http://$(oc get route | grep -i core-kafdrop | awk '{print $2}')/topic/lpr/messages?partition=0&offset=0&count=100&keyFormat=DEFAULT&format=DEFAULT"
----

=== Fee Calculation

For calculating the toll and pollution fee, there are two cases that we have covered:

* When any vehicle enters the ULEZ, a certain fee (aka toll fee) must be applied to that vehicle
* If the vehicle model is too old (older than 2014), apply addition fee (aka pollution fee) on that vehicle

Deploy the fee calculation component, using the following commands

[source,bash]
----
oc create -f fee_calculation/is_fee_calculation.yaml
oc create -f fee_calculation/bc_fee_calculation.yaml
oc create -f fee_calculation/cronjob_fee_calculation.yaml
----

=== Secor

Secor is the component that will listen to the Kafka Stream and write the aggregated data to an object Bucket.

* `secor/1_obc_secor.yaml`: Bucket to store the streamed data
* `secor/2_zookeeper_entrance.yaml`: Connection to the Kafka-Core instance
* `secor/3_secor.yaml`: Deploys the Secor instance

=== Superset - Trino

TODO: Manual deployments

Open Data Hub will allow us to easily deploy SuperSet and Trino.

IMPORTANT: Before you apply `opendatahub/kfdef.yaml` make sure to replace s3 endpoint with RWG IP, using the following command

// TODO: Check the s3.data.local deployment

[source,bash]
----
RGW_IP=$(oc get svc -n openshift-storage | grep -i rgw | awk '{print $3}')
sed -i 's/s3.data.local/'$RGW_IP'/g' opendatahub/kfdef.yaml
----

* `opendatahub/kfdef.yaml`: Deploys an Open Data Hub instance with the needed components

Once the components are running (check the pods!) you can connect to the ODH dasboard to launch Superset or Grafana. The Route can be found in the OpenShift UI or like this:

[source,bash]
----
echo "https://$(oc get route | grep -i odh-dashboard | awk '{print $2}')"
----

==== Superset

* For superset to establish connection with PostgreSQL, set the credentials in `superset-dasboard.yaml` file

[source, bash]
----
sed -i "s/DB_USER/dbadmin/" superset/config/superset-datasources.yaml
sed -i "s/DB_PASSWORD/dbpassword/" superset/config/superset-datasources.yaml
sed -i "s/DB_NAME/pgdb/" superset/config/superset-datasources.yaml
----

* Transfer the DataSources configuration file into the Superset pod.

[source,bash]
----
oc rsync superset/config $(oc get pod | grep superset- | awk '{print $1}'):/tmp
----

* Import the datasources into Superset (PostgreSQL and Hive from Trino)

[source,bash]
----
oc exec $(oc get pod | grep superset- | awk '{print $1}') -- superset import_datasources -p /tmp/config/superset-datasources.yaml
----

* Log into Superset you can use admin / admin (unless you have modified it into the ODH KfDef).
* From the Settings menu (top right), import the example dasboard from the file `dashboard/dashboard.json`

==== Trino

Once the trino-coordinator pod is running, connect to trino using trino-cli

[source,bash]
----
wget https://repo1.maven.org/maven2/io/trino/trino-cli/358/trino-cli-358-executable.jar -O trino
chmod +x trino
oc port-forward svc/trino-service 8080:8080
./trino --server localhost:8080 --catalog hive --schema default
----

From the Trino prompt, create schema and table

IMPORTANT: Before you execute the command to create schema and table , make sure to replace the bucket name with your bucket. To grab bucket name execute `oc get obc secor-obc -o json | jq -r .spec.bucketName`

[source,sql]
----
CREATE SCHEMA hive.odf WITH (location = 's3a://replace_with_secor_bucket_name/');

CREATE TABLE IF NOT EXISTS hive.odf.event(event_timestamp timestamp, event_id varchar, event_vehicle_detected_plate_number varchar, event_vehicle_detected_lat varchar, event_vehicle_detected_long varchar, event_vehicle_lpn_detection_status varchar, stationa1 boolean, stationa5201 boolean, stationa13 boolean, stationa2 boolean, stationa23 boolean, stationb313 boolean, stationa4202 boolean, stationa41 boolean, stationb504 boolean, dt varchar) with ( external_location = 's3a://replace_with_secor_bucket_name/raw_logs/lpr/', format = 'ORC', partitioned_by=ARRAY['dt']);

CALL system.sync_partition_metadata(schema_name=>'odf', table_name=>'event', mode=>'FULL');

SELECT event_timestamp,event_vehicle_detected_plate_number,event_vehicle_lpn_detection_status FROM hive.odf.event LIMIT 10;
----


=== Grafana

Grafana will allow us to create dashbord to visualize the data workflow (Ops dashboard) and the Business Application itself (Main dashboard). All the deployments are taken care of by the Grafana operator deployed previously (see requirements).

* PGSQL Source to retrieve the events and vehicle data

.Retrieve the secrets, process the template, and apply the configuration
[source,bash]
----
oc process -f grafana/grafana-pgsql-datasource.yaml -p db_database=$(oc get secret/postgresql -o yaml | grep " database-name:" | awk '{ print $2 }' - | base64 -d) -p db_user=$(oc get secret/postgresql -o yaml | grep " database-user:" | awk '{ print $2 }' - | base64 -d) -p db_password=$(oc get secret/postgresql -o yaml | grep " database-password:" | awk '{ print $2 }' - | base64 -d) | oc apply -f -
----

* Prometheus Data Source to retrieve the CPU and RAM metrics

Our Grafana dashboard wil connect to the main OpenShift Prometheus instance to retrieve CPU and RAM information. To enable this, follow those steps:

.Grant the Grafana Service Account the cluster-monitoring-view cluster role:
[source,bash]
----
oc adm policy add-cluster-role-to-user cluster-monitoring-view -z grafana-serviceaccount
----

.Retrieve the bearer token used to authenticate to Prometheus:
[source,bash]
----
export bearer_token=$(oc serviceaccounts get-token grafana-serviceaccount)
----

.Deploy the Prometheus data source by using the template and substituting the bearer token:
[source,bash]
----
sed 's/BEARER_TOKEN/'$bearer_token'/' grafana/grafana-prometheus-datasource.yaml | oc apply -f -
----

You can now apply the two last files:

* Main application dashboard

.Retrieve the image server url, process the template, and apply the configuration
[source,bash]
----
oc process -f grafana/grafana-main-dashboard.yaml -p image_server_host=$(oc get route | grep -i image-server | awk '{print $2}') | oc apply -f -
----

* `grafana/grafana-pipeline-cpu-dashboard.yaml`: CPU Ops dashboard
* `grafana/grafana-pipeline-ram-dashboard.yaml`: RAM Ops dashboard
>>>>>>> main
